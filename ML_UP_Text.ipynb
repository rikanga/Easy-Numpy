{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_UP_Text",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNHWFCR3HwidGdMJwPVSx4i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rikanga/Easy-Numpy/blob/main/ML_UP_Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling Text\n",
        "\n",
        "## 6.1 Cleaning Text\n",
        "\n",
        "**Problem**\n",
        "\n",
        "You have some unstructured text data and want to complete some basic cleaning.\n",
        "\n",
        "**Solution**\n",
        "\n",
        "Most basic text cleaning operations should only replace Python’s core string opera‐\n",
        "tions, in particular strip , replace , and split :"
      ],
      "metadata": {
        "id": "KX7Hm1rXHXCF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qX3XfhLUHNfK"
      },
      "outputs": [],
      "source": [
        "# Create text\n",
        "text_data = [\n",
        "             \"   Interrobang. By Aishwarya Henriette   \",\n",
        "             \"Parking And Going. By Karl Gautier\",\n",
        "             \"   Today Is The night. By Jarek Prakash   \"\n",
        "             ]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Strip whiitespace\n",
        "strip_whitespace = [strip.strip() for strip in text_data]"
      ],
      "metadata": {
        "id": "3xu-BR5VIm_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View strip_whitespace\n",
        "strip_whitespace"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sDS5THGI3e-",
        "outputId": "3b4c4a68-2b43-49c7-8dc7-89f065863cef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Interrobang. By Aishwarya Henriette',\n",
              " 'Parking And Going. By Karl Gautier',\n",
              " 'Today Is The night. By Jarek Prakash']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stop\n",
        "remove_stop = [string.replace('.', '') for string in strip_whitespace]"
      ],
      "metadata": {
        "id": "HXhpNPXZI8Ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View remove_stop\n",
        "remove_stop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hjlPoUYJvEF",
        "outputId": "d251ff92-4fc5-48f8-dc0a-f55f692c5374"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Interrobang By Aishwarya Henriette',\n",
              " 'Parking And Going By Karl Gautier',\n",
              " 'Today Is The night By Jarek Prakash']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create function\n",
        "def capitalizer(string: str):\n",
        "  return string.upper()"
      ],
      "metadata": {
        "id": "jzEZ0nmlJyKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply function\n",
        "[capitalizer(string) for string in remove_stop]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2lp5fFkKH9l",
        "outputId": "b6a4fbdb-6310-4ef7-9104-2e5533196377"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['INTERROBANG BY AISHWARYA HENRIETTE',\n",
              " 'PARKING AND GOING BY KARL GAUTIER',\n",
              " 'TODAY IS THE NIGHT BY JAREK PRAKASH']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# USING REGULAR EXPRESSION\n",
        "# Load libray\n",
        "import re\n",
        "\n",
        "# Define the function\n",
        "def replace_letters_with_X(string:str):\n",
        "  return re.sub(r'[a-zA-Z]','X', string)"
      ],
      "metadata": {
        "id": "nbzezICZKQ1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply function with re pattern\n",
        "[replace_letters_with_X(string) for string in remove_stop]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JP6tRKagLdJQ",
        "outputId": "1c9c6882-25a9-4c41-f656-6a39ffc0e7d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['XXXXXXXXXXX XX XXXXXXXXX XXXXXXXXX',\n",
              " 'XXXXXXX XXX XXXXX XX XXXX XXXXXXX',\n",
              " 'XXXXX XX XXX XXXXX XX XXXXX XXXXXXX']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2 Parsing and Cleaning HTML\n",
        "\n",
        "**Problem**\n",
        "\n",
        "You have text data with HTML elements and want to extract just the text.\n",
        "\n",
        "**Solution**\n",
        "\n",
        "Use Beautiful Soup’s extensive set of options to parse and extract from HTML:"
      ],
      "metadata": {
        "id": "-MHlXvQaQ-QW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bs4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZjRXHGHQRkL",
        "outputId": "175453a1-761f-4b07-db67-4f660ff397ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4) (4.6.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libray\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "noOxz4rfLotC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create some HTML code\n",
        "html = \"\"\"\n",
        "<div class='full_name'><span style='font-weight:bold'>\n",
        "Masego</span> Azra</div>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "YAdY7wnuQMVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parse html\n",
        "soup = BeautifulSoup(html, 'lxml')"
      ],
      "metadata": {
        "id": "GMY2R-Z9Rq5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQgM62EBR8IF",
        "outputId": "4a073a74-1cda-485e-c651-a10861bb10d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<html><body><div class=\"full_name\"><span style=\"font-weight:bold\">\n",
              "Masego</span> Azra</div>\n",
              "</body></html>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the class with full_name, show text\n",
        "soup.find(\"div\", {'class':'full_name'}).text.strip()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UQcfFsVISIwk",
        "outputId": "65fa3688-664f-423e-fc4d-078d1c0d381d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Masego Azra'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "soup.find('span', {\"style\":'font-weight:bold'}).text.strip()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZBxXdshUSeYa",
        "outputId": "95e2d2ac-c3a0-44b1-acb0-4971cf650c77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Masego'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "ue_eWaYyT3JE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3 Removing Punctuation\n",
        "\n",
        "**Problem**\n",
        "\n",
        "You have a feature of text data and want to remove punctuation.\n",
        "\n",
        "**Solution**\n",
        "\n",
        "Define a function that uses translate with a dictionary of punctuation characters:"
      ],
      "metadata": {
        "id": "tZs2-KR0WH56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "import sys\n",
        "import unicodedata"
      ],
      "metadata": {
        "id": "itBzmKs9VTg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create text\n",
        "text_data = [\n",
        "             'Hi!!!! I. Love. This. Song....',\n",
        "             '10000% Agree!!!! #LoveIT',\n",
        "             'Right?!?!']"
      ],
      "metadata": {
        "id": "rolQTP82XUOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_found = [re.findall(r'[a-zA-Z0-9]+', x) for x in text_data]\n",
        "word_found"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pl9fbFOMZmjY",
        "outputId": "eefb57a0-1871-4887-f182-7e99dbaa1dd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Hi', 'I', 'Love', 'This', 'Song'], ['10000', 'Agree', 'LoveIT'], ['Right']]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_data = [','.join(x) for x in word_found]"
      ],
      "metadata": {
        "id": "gf_-7jsSaLAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[string.replace(',', ' ') for string in new_data]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YmhxBG4c3sg",
        "outputId": "72953eab-f7e0-41ec-db56-b8126de060d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hi I Love This Song', '10000 Agree LoveIT', 'Right']"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.4 Tokenizing Text\n",
        "\n",
        "**Problem**\n",
        "\n",
        "You have text and want to break it up into individual words.\n",
        "\n",
        "**Solution**\n",
        "\n",
        "Natural Language Toolkit for Python (NLTK) has a powerful set of text manipulation\n",
        "operations, including word tokenizing:"
      ],
      "metadata": {
        "id": "mBAHrVXtd0HY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load library\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "vf5nZUysdqnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create text\n",
        "string = \"The science of today is the technology of tomorrow\""
      ],
      "metadata": {
        "id": "JZYCG6yfjiGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the library\n",
        "import nltk"
      ],
      "metadata": {
        "id": "Ie_awMSNkGuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dowload 'punkt\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2inFPkjkQXE",
        "outputId": "ec22ebe3-7f96-4a64-aa14-1df6e9247db8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the string\n",
        "word_tokenize(string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c409pIqSkiVq",
        "outputId": "beb907ff-a99b-4ff7-975b-c96ee3d782fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'science', 'of', 'today', 'is', 'the', 'technology', 'of', 'tomorrow']"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(\"Bonjour tout le monde, je suis chez moi à la maison. Content de vous parler\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41QBZpp6lK3N",
        "outputId": "40992ff5-1892-40f6-b0a6-37f6b56c863d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Bonjour',\n",
              " 'tout',\n",
              " 'le',\n",
              " 'monde',\n",
              " ',',\n",
              " 'je',\n",
              " 'suis',\n",
              " 'chez',\n",
              " 'moi',\n",
              " 'à',\n",
              " 'la',\n",
              " 'maison',\n",
              " '.',\n",
              " 'Content',\n",
              " 'de',\n",
              " 'vous',\n",
              " 'parler']"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also tokenize in sentence"
      ],
      "metadata": {
        "id": "Wcr6cPKnm2t8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "NI3IXtozmn0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize in the sentence\n",
        "sent_tokenize(string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5s0AzIKpm-Pi",
        "outputId": "ce3f5096-ce9f-4bd0-b7a8-c3baaa8e0683"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The science of today is the technology of tomorrow']"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.5 Removing Stop Word\n",
        "\n",
        "**Problem**\n",
        "\n",
        "Given tokenized text data, you want to remove extremely common words (e.g., a, is,\n",
        "of, on) that contain little informational value.\n",
        "\n",
        "After tokenize we can remove stop words\n",
        "**Solution**\n",
        "\n",
        "Use NLTK’s stopwords :"
      ],
      "metadata": {
        "id": "CZH5KBQqoC2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "xolpgx4znDbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NUHqThppffV",
        "outputId": "66315a0c-43dd-4911-ddbf-000e123f5ec6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create word tokens\n",
        "tokenized_words = ['i',\n",
        "'am',\n",
        "'going',\n",
        "'to',\n",
        "'go',\n",
        "'to',\n",
        "'the',\n",
        "'store',\n",
        "'and',\n",
        "'park']"
      ],
      "metadata": {
        "id": "NcOohUzzqKac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWxSI_b-qeNx",
        "outputId": "167d79b7-07b2-43d3-8322-6cf133a3744a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'am', 'going', 'to', 'go', 'to', 'the', 'store', 'and', 'park']"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load stop words\n",
        "stop_words = stopwords.words('english')"
      ],
      "metadata": {
        "id": "Lh9a7QMQqf_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyxPTAkkqt3w",
        "outputId": "0ea8c8b9-d3f7-4925-c39a-a6e383095d11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stop word\n",
        "[word for word in tokenized_words if word not in stop_words]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FP0Va5iq1gH",
        "outputId": "c04b018c-6024-43ca-d132-2a593b631f19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['going', 'go', 'store', 'park']"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.6 Stemming Words(Mots radicaux)\n",
        "\n",
        "**Problème**\n",
        "\n",
        "Vous avez des mots symbolisés(tokenized) et souhaitez les convertir dans leurs formes racine.\n",
        "\n",
        "**Solution**\n",
        "\n",
        "Utiliser le PorterStemer de NLTK\n"
      ],
      "metadata": {
        "id": "qh5is1Pf93iy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load library\n",
        "from nltk.stem.porter import PorterStemmer"
      ],
      "metadata": {
        "id": "oOZq4KH3rR63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create word tokens\n",
        "tokenized_words = ['i', 'am', 'humbled', 'by', 'this', 'traditional', 'meeting']"
      ],
      "metadata": {
        "id": "DA7manJeDjUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create stemmer\n",
        "porter = PorterStemmer()"
      ],
      "metadata": {
        "id": "JD78mU-TENhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply stemmer\n",
        "[porter.stem(word) for word in tokenized_words]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dC8bTM0pEVb_",
        "outputId": "2331ebdb-9bbe-41ab-f238-53439b9c946e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'am', 'humbl', 'by', 'thi', 'tradit', 'meet']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.7 Tagging Parts of Speech\n",
        "\n",
        "**Problem**\n",
        "\n",
        "You have text data and want to tag each word or character with its part of speech.\n",
        "\n",
        "**Solution**\n",
        "\n",
        "Use NLTK’s pre-trained parts-of-speech tagger"
      ],
      "metadata": {
        "id": "u1wSd-6gG3v9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import word_tokenize\n",
        "from nltk import pos_tag"
      ],
      "metadata": {
        "id": "wVGBFVjOG0ag"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create text\n",
        "text_data = \"Chris loved outdoor running\""
      ],
      "metadata": {
        "id": "u6JqrlZwEodV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk; nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxVAQdz9KDcb",
        "outputId": "a80a4b51-7624-4219-80f4-4676b644b0f3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the word\n",
        "word_tokenized = word_tokenize(text_data)\n",
        "word_tokenized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCK1bU1pJzOz",
        "outputId": "979870fd-ad87-4e33-fcb2-49f41d641830"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Chris', 'loved', 'outdoor', 'running']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1t-tQNyTL9dG",
        "outputId": "d9ea59d9-3a92-4406-e383-3a734940e190"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use pre-trained part of speech tagger\n",
        "text_tag = pos_tag(word_tokenized)"
      ],
      "metadata": {
        "id": "veC7fM2FJ-1d"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the text tag\n",
        "text_tag"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSd4cCn_L45a",
        "outputId": "2e96cd8e-18e0-42cd-8043-2909d42bc1c9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Chris', 'NNP'), ('loved', 'VBD'), ('outdoor', 'RP'), ('running', 'VBG')]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter words to have just the noun\n",
        "[word for word, tag in text_tag if tag in ['NN', 'NNS', 'NNP', 'NNPS']]"
      ],
      "metadata": {
        "id": "HuK4MSWcMFh2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04987bbb-6d3a-4fb7-f64b-59f62e0fae9b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Chris']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# OTHER EXAMPLE\n",
        "# Create text\n",
        "tweets = [\n",
        "          'I am eating a burrito for breakfast',\n",
        "          'Political science is amazing field',\n",
        "          'San Francisco is an awesome city'\n",
        "]"
      ],
      "metadata": {
        "id": "jI7NYjLXlqKz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create list\n",
        "tagged_tweets = []"
      ],
      "metadata": {
        "id": "NIFjJLI1lSOR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tag each word and each tweet\n",
        "for tweet in tweets:\n",
        "  tweet_tag = nltk.pos_tag(word_tokenize(tweet))\n",
        "  tagged_tweets.append([tag for word, tag in tweet_tag])"
      ],
      "metadata": {
        "id": "slBjDHB3mBtP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_tweets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTK75jrDmdEk",
        "outputId": "693fc327-478f-426e-a34e-ff5df95770ea"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['PRP', 'VBP', 'VBG', 'DT', 'NN', 'IN', 'NN'],\n",
              " ['JJ', 'NN', 'VBZ', 'JJ', 'NN'],\n",
              " ['NNP', 'NNP', 'VBZ', 'DT', 'JJ', 'NN']]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the library\n",
        "from sklearn.preprocessing import MultiLabelBinarizer"
      ],
      "metadata": {
        "id": "0qiJq43lmuSN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use one-hot encoding to convert the tags into features\n",
        "one_hot_multi = MultiLabelBinarizer()\n",
        "one_hot_multi.fit_transform(tagged_tweets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q794nAx4oxKu",
        "outputId": "ac2349c2-af2c-40be-a5dd-0831d2d332fc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 0, 1, 0, 1, 1, 1, 0],\n",
              "       [0, 0, 1, 1, 0, 0, 0, 0, 1],\n",
              "       [1, 0, 1, 1, 1, 0, 0, 0, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the feature names\n",
        "one_hot_multi.classes_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDLYvrVbo6kV",
        "outputId": "0372833b-6082-45b1-802b-10cc4edbbe10"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['DT', 'IN', 'JJ', 'NN', 'NNP', 'PRP', 'VBG', 'VBP', 'VBZ'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load library\n",
        "from nltk.corpus import brown\n",
        "from nltk.tag import UnigramTagger\n",
        "from nltk.tag import BigramTagger\n",
        "from nltk.tag import TrigramTagger"
      ],
      "metadata": {
        "id": "roj5UKRapKzS"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get some text from the Brown Corpus, broken into sentences\n",
        "nltk.download('brown')\n",
        "sentences = brown.tagged_sents(categories='news')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUcc7aUc-nSk",
        "outputId": "f3b141f4-a5a7-4ce5-8964-03e411a36096"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into 4000 sentences for training and 623 for testing\n",
        "train = sentences[:4000]\n",
        "test = sentences[4000:]"
      ],
      "metadata": {
        "id": "4EK4NWumAXjl"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3cGx6NPAezv",
        "outputId": "7a594446-96c8-43d7-bd48-3a6c3476cea5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('The', 'AT'),\n",
              "  ('Fulton', 'NP-TL'),\n",
              "  ('County', 'NN-TL'),\n",
              "  ('Grand', 'JJ-TL'),\n",
              "  ('Jury', 'NN-TL'),\n",
              "  ('said', 'VBD'),\n",
              "  ('Friday', 'NR'),\n",
              "  ('an', 'AT'),\n",
              "  ('investigation', 'NN'),\n",
              "  ('of', 'IN'),\n",
              "  (\"Atlanta's\", 'NP$'),\n",
              "  ('recent', 'JJ'),\n",
              "  ('primary', 'NN'),\n",
              "  ('election', 'NN'),\n",
              "  ('produced', 'VBD'),\n",
              "  ('``', '``'),\n",
              "  ('no', 'AT'),\n",
              "  ('evidence', 'NN'),\n",
              "  (\"''\", \"''\"),\n",
              "  ('that', 'CS'),\n",
              "  ('any', 'DTI'),\n",
              "  ('irregularities', 'NNS'),\n",
              "  ('took', 'VBD'),\n",
              "  ('place', 'NN'),\n",
              "  ('.', '.')],\n",
              " [('The', 'AT'),\n",
              "  ('jury', 'NN'),\n",
              "  ('further', 'RBR'),\n",
              "  ('said', 'VBD'),\n",
              "  ('in', 'IN'),\n",
              "  ('term-end', 'NN'),\n",
              "  ('presentments', 'NNS'),\n",
              "  ('that', 'CS'),\n",
              "  ('the', 'AT'),\n",
              "  ('City', 'NN-TL'),\n",
              "  ('Executive', 'JJ-TL'),\n",
              "  ('Committee', 'NN-TL'),\n",
              "  (',', ','),\n",
              "  ('which', 'WDT'),\n",
              "  ('had', 'HVD'),\n",
              "  ('over-all', 'JJ'),\n",
              "  ('charge', 'NN'),\n",
              "  ('of', 'IN'),\n",
              "  ('the', 'AT'),\n",
              "  ('election', 'NN'),\n",
              "  (',', ','),\n",
              "  ('``', '``'),\n",
              "  ('deserves', 'VBZ'),\n",
              "  ('the', 'AT'),\n",
              "  ('praise', 'NN'),\n",
              "  ('and', 'CC'),\n",
              "  ('thanks', 'NNS'),\n",
              "  ('of', 'IN'),\n",
              "  ('the', 'AT'),\n",
              "  ('City', 'NN-TL'),\n",
              "  ('of', 'IN-TL'),\n",
              "  ('Atlanta', 'NP-TL'),\n",
              "  (\"''\", \"''\"),\n",
              "  ('for', 'IN'),\n",
              "  ('the', 'AT'),\n",
              "  ('manner', 'NN'),\n",
              "  ('in', 'IN'),\n",
              "  ('which', 'WDT'),\n",
              "  ('the', 'AT'),\n",
              "  ('election', 'NN'),\n",
              "  ('was', 'BEDZ'),\n",
              "  ('conducted', 'VBN'),\n",
              "  ('.', '.')],\n",
              " [('The', 'AT'),\n",
              "  ('September-October', 'NP'),\n",
              "  ('term', 'NN'),\n",
              "  ('jury', 'NN'),\n",
              "  ('had', 'HVD'),\n",
              "  ('been', 'BEN'),\n",
              "  ('charged', 'VBN'),\n",
              "  ('by', 'IN'),\n",
              "  ('Fulton', 'NP-TL'),\n",
              "  ('Superior', 'JJ-TL'),\n",
              "  ('Court', 'NN-TL'),\n",
              "  ('Judge', 'NN-TL'),\n",
              "  ('Durwood', 'NP'),\n",
              "  ('Pye', 'NP'),\n",
              "  ('to', 'TO'),\n",
              "  ('investigate', 'VB'),\n",
              "  ('reports', 'NNS'),\n",
              "  ('of', 'IN'),\n",
              "  ('possible', 'JJ'),\n",
              "  ('``', '``'),\n",
              "  ('irregularities', 'NNS'),\n",
              "  (\"''\", \"''\"),\n",
              "  ('in', 'IN'),\n",
              "  ('the', 'AT'),\n",
              "  ('hard-fought', 'JJ'),\n",
              "  ('primary', 'NN'),\n",
              "  ('which', 'WDT'),\n",
              "  ('was', 'BEDZ'),\n",
              "  ('won', 'VBN'),\n",
              "  ('by', 'IN'),\n",
              "  ('Mayor-nominate', 'NN-TL'),\n",
              "  ('Ivan', 'NP'),\n",
              "  ('Allen', 'NP'),\n",
              "  ('Jr.', 'NP'),\n",
              "  ('.', '.')],\n",
              " [('``', '``'),\n",
              "  ('Only', 'RB'),\n",
              "  ('a', 'AT'),\n",
              "  ('relative', 'JJ'),\n",
              "  ('handful', 'NN'),\n",
              "  ('of', 'IN'),\n",
              "  ('such', 'JJ'),\n",
              "  ('reports', 'NNS'),\n",
              "  ('was', 'BEDZ'),\n",
              "  ('received', 'VBN'),\n",
              "  (\"''\", \"''\"),\n",
              "  (',', ','),\n",
              "  ('the', 'AT'),\n",
              "  ('jury', 'NN'),\n",
              "  ('said', 'VBD'),\n",
              "  (',', ','),\n",
              "  ('``', '``'),\n",
              "  ('considering', 'IN'),\n",
              "  ('the', 'AT'),\n",
              "  ('widespread', 'JJ'),\n",
              "  ('interest', 'NN'),\n",
              "  ('in', 'IN'),\n",
              "  ('the', 'AT'),\n",
              "  ('election', 'NN'),\n",
              "  (',', ','),\n",
              "  ('the', 'AT'),\n",
              "  ('number', 'NN'),\n",
              "  ('of', 'IN'),\n",
              "  ('voters', 'NNS'),\n",
              "  ('and', 'CC'),\n",
              "  ('the', 'AT'),\n",
              "  ('size', 'NN'),\n",
              "  ('of', 'IN'),\n",
              "  ('this', 'DT'),\n",
              "  ('city', 'NN'),\n",
              "  (\"''\", \"''\"),\n",
              "  ('.', '.')],\n",
              " [('The', 'AT'),\n",
              "  ('jury', 'NN'),\n",
              "  ('said', 'VBD'),\n",
              "  ('it', 'PPS'),\n",
              "  ('did', 'DOD'),\n",
              "  ('find', 'VB'),\n",
              "  ('that', 'CS'),\n",
              "  ('many', 'AP'),\n",
              "  ('of', 'IN'),\n",
              "  (\"Georgia's\", 'NP$'),\n",
              "  ('registration', 'NN'),\n",
              "  ('and', 'CC'),\n",
              "  ('election', 'NN'),\n",
              "  ('laws', 'NNS'),\n",
              "  ('``', '``'),\n",
              "  ('are', 'BER'),\n",
              "  ('outmoded', 'JJ'),\n",
              "  ('or', 'CC'),\n",
              "  ('inadequate', 'JJ'),\n",
              "  ('and', 'CC'),\n",
              "  ('often', 'RB'),\n",
              "  ('ambiguous', 'JJ'),\n",
              "  (\"''\", \"''\"),\n",
              "  ('.', '.')]]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nfNOrnsBD63",
        "outputId": "3570342a-02f7-4442-cb27-c78f5645bbdf"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('In', 'IN'),\n",
              "  (\"Ruth's\", 'NP$'),\n",
              "  ('day', 'NN'),\n",
              "  ('--', '--'),\n",
              "  ('and', 'CC'),\n",
              "  ('until', 'IN'),\n",
              "  ('this', 'DT'),\n",
              "  ('year', 'NN'),\n",
              "  ('--', '--'),\n",
              "  ('the', 'AT'),\n",
              "  ('schedule', 'NN'),\n",
              "  ('was', 'BEDZ'),\n",
              "  ('154', 'CD'),\n",
              "  ('games', 'NNS'),\n",
              "  ('.', '.')],\n",
              " [('Baseball', 'NN'),\n",
              "  ('commissioner', 'NN'),\n",
              "  ('Ford', 'NP'),\n",
              "  ('Frick', 'NP'),\n",
              "  ('has', 'HVZ'),\n",
              "  ('ruled', 'VBN'),\n",
              "  ('that', 'CS'),\n",
              "  (\"Ruth's\", 'NP$'),\n",
              "  ('record', 'NN'),\n",
              "  ('will', 'MD'),\n",
              "  ('remain', 'VB'),\n",
              "  ('official', 'JJ'),\n",
              "  ('unless', 'CS'),\n",
              "  ('it', 'PPS'),\n",
              "  ('is', 'BEZ'),\n",
              "  ('broken', 'VBN'),\n",
              "  ('in', 'IN'),\n",
              "  ('154', 'CD'),\n",
              "  ('games', 'NNS'),\n",
              "  ('.', '.')],\n",
              " [(')', ')')],\n",
              " [('``', '``'),\n",
              "  ('Even', 'RB'),\n",
              "  ('on', 'IN'),\n",
              "  ('the', 'AT'),\n",
              "  ('basis', 'NN'),\n",
              "  ('of', 'IN'),\n",
              "  ('154', 'CD'),\n",
              "  ('games', 'NNS'),\n",
              "  (',', ','),\n",
              "  ('this', 'DT'),\n",
              "  ('is', 'BEZ'),\n",
              "  ('the', 'AT'),\n",
              "  ('ideal', 'JJ'),\n",
              "  ('situation', 'NN'),\n",
              "  (\"''\", \"''\"),\n",
              "  (',', ','),\n",
              "  ('insists', 'VBZ'),\n",
              "  ('Hank', 'NP'),\n",
              "  ('Greenberg', 'NP'),\n",
              "  (',', ','),\n",
              "  ('now', 'RB'),\n",
              "  ('vice-president', 'NN'),\n",
              "  ('of', 'IN'),\n",
              "  ('the', 'AT'),\n",
              "  ('Chicago', 'NP-TL'),\n",
              "  ('White', 'NN-TL'),\n",
              "  ('Sox', 'NPS-TL'),\n",
              "  ('.', '.')],\n",
              " [('``', '``'),\n",
              "  ('It', 'PPS'),\n",
              "  ('has', 'HVZ'),\n",
              "  ('to', 'TO'),\n",
              "  ('be', 'BE'),\n",
              "  ('easier', 'JJR'),\n",
              "  ('with', 'IN'),\n",
              "  ('two', 'CD'),\n",
              "  ('of', 'IN'),\n",
              "  ('them', 'PPO'),\n",
              "  ('.', '.')]]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create backoff tagger\n",
        "unigram = UnigramTagger(train)\n",
        "bigram = BigramTagger(train, backoff=unigram)\n",
        "trigram = TrigramTagger(train, backoff=bigram)"
      ],
      "metadata": {
        "id": "4w8qqMWyBGJj"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show accuracy\n",
        "trigram.evaluate(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iruwkNrTCWHh",
        "outputId": "0b472726-35aa-4216-c827-74618b81932d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8174734002697437"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.8 Encoding Text as a Bag of Words\n",
        "\n",
        "**Problem**\n",
        "\n",
        "You have text data and want to create a set of features indicating the number of times\n",
        "an observation’s text contains a particular word.\n",
        "\n",
        "**Solution**\n",
        "\n",
        "Use scikit-learn’s CountVectorizer :"
      ],
      "metadata": {
        "id": "2bk2LCbYcpMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libray\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "146zb5HaF8lP"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create text\n",
        "text_data = np.array([\n",
        "                      'I love Brazil. Brazil!',\n",
        "                      'Sweden is best',\n",
        "                      'Germany beats both'])"
      ],
      "metadata": {
        "id": "HxDXPVPtd1kq"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Create the bag of words feature matrix\n",
        " count = CountVectorizer()\n",
        "\n",
        " bag_of_words = count.fit_transform(text_data)"
      ],
      "metadata": {
        "id": "G42DcgKweJzA"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show feature matrix\n",
        "bag_of_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKNefmRXenjK",
        "outputId": "ceb364b4-00a2-4e42-ab91-13a5d0fd6411"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<3x8 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 8 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Translate to array\n",
        "bag_of_words.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNASGxCIewb2",
        "outputId": "535add95-5e82-4295-ffc3-34532ae05440"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 2, 0, 0, 1, 0],\n",
              "       [0, 1, 0, 0, 0, 1, 0, 1],\n",
              "       [1, 0, 1, 0, 1, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the feature name\n",
        "print(count.get_feature_names())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKDS_Kohe76q",
        "outputId": "d5f776f2-645e-4f39-9a22-e691a5964dff"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['beats', 'best', 'both', 'brazil', 'germany', 'is', 'love', 'sweden']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Q-60WnNigdY",
        "outputId": "956c7ece-97fe-4f11-d9f3-15ceb1e825e1"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'beats': 0,\n",
              " 'best': 1,\n",
              " 'both': 2,\n",
              " 'brazil': 3,\n",
              " 'germany': 4,\n",
              " 'is': 5,\n",
              " 'love': 6,\n",
              " 'sweden': 7}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Aj8eZp35fadQ"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(bag_of_words.toarray(), columns=count.get_feature_names())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "qKUO3BQvfrOt",
        "outputId": "aeeab0cd-216f-43bc-c194-15c08a776b05"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   beats  best  both  brazil  germany  is  love  sweden\n",
              "0      0     0     0       2        0   0     1       0\n",
              "1      0     1     0       0        0   1     0       1\n",
              "2      1     0     1       0        1   0     0       0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-80b2a03f-b5dd-4dcb-a32a-10cde347a002\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>beats</th>\n",
              "      <th>best</th>\n",
              "      <th>both</th>\n",
              "      <th>brazil</th>\n",
              "      <th>germany</th>\n",
              "      <th>is</th>\n",
              "      <th>love</th>\n",
              "      <th>sweden</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-80b2a03f-b5dd-4dcb-a32a-10cde347a002')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-80b2a03f-b5dd-4dcb-a32a-10cde347a002 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-80b2a03f-b5dd-4dcb-a32a-10cde347a002');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(count.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urLWohW0inzU",
        "outputId": "d69e7f4b-cf06-4aaf-cfde-693aa796aa33"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "love       6\n",
              "brazil     3\n",
              "sweden     7\n",
              "is         5\n",
              "best       1\n",
              "germany    4\n",
              "beats      0\n",
              "both       2\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.9 Weighting Word Importance\n",
        "\n",
        "**Problem**\n",
        "\n",
        "You want a bag of words, but with words weighted by their importance to an observation.\n",
        "\n",
        "**Solution**\n",
        "\n",
        "Compare the frequency of the word in a document (a tweet, movie review, speech\n",
        "transcript, etc.) with the frequency of the word in all other documents using term\n",
        "frequency-inverse document frequency (tf-idf). scikit-learn makes this easy with\n",
        "TfidfVectorizer :"
      ],
      "metadata": {
        "id": "Lo-jrcmDgZJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load library\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "c3f9oaT_fzgf"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create text\n",
        "text_data = np.array([\n",
        "                      'I love Brazil. Brazil!',\n",
        "                      'Sweden is best',\n",
        "                      'Germany beats both'])\n",
        "text_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xoia6OzkITW",
        "outputId": "e2337b72-4780-4d66-a980-9d15d47aa49c"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['I love Brazil. Brazil!', 'Sweden is best', 'Germany beats both'],\n",
              "      dtype='<U22')"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the tf-idf feature matrix\n",
        "tfidf = TfidfVectorizer()\n",
        "feature_matrix = tfidf.fit_transform(text_data)"
      ],
      "metadata": {
        "id": "MLRhQmuPlRuJ"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show tf-idf feature matrix\n",
        "feature_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MteK14XHlr6_",
        "outputId": "3b58d31f-1cab-4f74-fa48-5ade01b6cc95"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<3x8 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 8 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# show tf-idf feature matrix as dense matrix\n",
        "feature_matrix.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yS49yErLl3-Y",
        "outputId": "1583e83f-883c-43d8-8ff7-fd1e32a7c1a3"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.89442719, 0.        ,\n",
              "        0.        , 0.4472136 , 0.        ],\n",
              "       [0.        , 0.57735027, 0.        , 0.        , 0.        ,\n",
              "        0.57735027, 0.        , 0.57735027],\n",
              "       [0.57735027, 0.        , 0.57735027, 0.        , 0.57735027,\n",
              "        0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On compare ici la fréquence du mot dans tous les autres documents"
      ],
      "metadata": {
        "id": "xP41qJjkmW4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show feature names\n",
        "tfidf.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8xt-VUmmDnS",
        "outputId": "d835fbff-2a03-40aa-ed44-65c35e81dd41"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'beats': 0,\n",
              " 'best': 1,\n",
              " 'both': 2,\n",
              " 'brazil': 3,\n",
              " 'germany': 4,\n",
              " 'is': 5,\n",
              " 'love': 6,\n",
              " 'sweden': 7}"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZipKEPuPm36n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}