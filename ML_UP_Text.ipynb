{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_UP_Text",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOLTZ2jDQnZyGU7jkqxs5T2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rikanga/Easy-Numpy/blob/main/ML_UP_Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling Text\n",
        "\n",
        "## 6.1 Cleaning Text\n",
        "\n",
        "**Problem**\n",
        "\n",
        "You have some unstructured text data and want to complete some basic cleaning.\n",
        "\n",
        "**Solution**\n",
        "\n",
        "Most basic text cleaning operations should only replace Python’s core string opera‐\n",
        "tions, in particular strip , replace , and split :"
      ],
      "metadata": {
        "id": "KX7Hm1rXHXCF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qX3XfhLUHNfK"
      },
      "outputs": [],
      "source": [
        "# Create text\n",
        "text_data = [\n",
        "             \"   Interrobang. By Aishwarya Henriette   \",\n",
        "             \"Parking And Going. By Karl Gautier\",\n",
        "             \"   Today Is The night. By Jarek Prakash   \"\n",
        "             ]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Strip whiitespace\n",
        "strip_whitespace = [strip.strip() for strip in text_data]"
      ],
      "metadata": {
        "id": "3xu-BR5VIm_x"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View strip_whitespace\n",
        "strip_whitespace"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sDS5THGI3e-",
        "outputId": "3b4c4a68-2b43-49c7-8dc7-89f065863cef"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Interrobang. By Aishwarya Henriette',\n",
              " 'Parking And Going. By Karl Gautier',\n",
              " 'Today Is The night. By Jarek Prakash']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stop\n",
        "remove_stop = [string.replace('.', '') for string in strip_whitespace]"
      ],
      "metadata": {
        "id": "HXhpNPXZI8Ey"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View remove_stop\n",
        "remove_stop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hjlPoUYJvEF",
        "outputId": "d251ff92-4fc5-48f8-dc0a-f55f692c5374"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Interrobang By Aishwarya Henriette',\n",
              " 'Parking And Going By Karl Gautier',\n",
              " 'Today Is The night By Jarek Prakash']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create function\n",
        "def capitalizer(string: str):\n",
        "  return string.upper()"
      ],
      "metadata": {
        "id": "jzEZ0nmlJyKe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply function\n",
        "[capitalizer(string) for string in remove_stop]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2lp5fFkKH9l",
        "outputId": "b6a4fbdb-6310-4ef7-9104-2e5533196377"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['INTERROBANG BY AISHWARYA HENRIETTE',\n",
              " 'PARKING AND GOING BY KARL GAUTIER',\n",
              " 'TODAY IS THE NIGHT BY JAREK PRAKASH']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# USING REGULAR EXPRESSION\n",
        "# Load libray\n",
        "import re\n",
        "\n",
        "# Define the function\n",
        "def replace_letters_with_X(string:str):\n",
        "  return re.sub(r'[a-zA-Z]','X', string)"
      ],
      "metadata": {
        "id": "nbzezICZKQ1Y"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply function with re pattern\n",
        "[replace_letters_with_X(string) for string in remove_stop]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JP6tRKagLdJQ",
        "outputId": "1c9c6882-25a9-4c41-f656-6a39ffc0e7d5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['XXXXXXXXXXX XX XXXXXXXXX XXXXXXXXX',\n",
              " 'XXXXXXX XXX XXXXX XX XXXX XXXXXXX',\n",
              " 'XXXXX XX XXX XXXXX XX XXXXX XXXXXXX']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2 Parsing and Cleaning HTML\n",
        "\n",
        "**Problem**\n",
        "\n",
        "You have text data with HTML elements and want to extract just the text.\n",
        "\n",
        "**Solution**\n",
        "\n",
        "Use Beautiful Soup’s extensive set of options to parse and extract from HTML:"
      ],
      "metadata": {
        "id": "-MHlXvQaQ-QW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bs4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZjRXHGHQRkL",
        "outputId": "e80b15b1-171c-4a2c-9d05-54d0a889b61d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4) (4.6.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libray\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "noOxz4rfLotC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create some HTML code\n",
        "html = \"\"\"\n",
        "<div class='full_name'><span style='font-weight:bold'>\n",
        "Masego</span> Azra</div>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "YAdY7wnuQMVO"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parse html\n",
        "soup = BeautifulSoup(html, 'lxml')"
      ],
      "metadata": {
        "id": "GMY2R-Z9Rq5S"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQgM62EBR8IF",
        "outputId": "4a073a74-1cda-485e-c651-a10861bb10d4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<html><body><div class=\"full_name\"><span style=\"font-weight:bold\">\n",
              "Masego</span> Azra</div>\n",
              "</body></html>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the class with full_name, show text\n",
        "soup.find(\"div\", {'class':'full_name'}).text.strip()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UQcfFsVISIwk",
        "outputId": "65fa3688-664f-423e-fc4d-078d1c0d381d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Masego Azra'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "soup.find('span', {\"style\":'font-weight:bold'}).text.strip()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZBxXdshUSeYa",
        "outputId": "95e2d2ac-c3a0-44b1-acb0-4971cf650c77"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Masego'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "ue_eWaYyT3JE"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3 Removing Punctuation\n",
        "\n",
        "**Problem**\n",
        "\n",
        "You have a feature of text data and want to remove punctuation.\n",
        "\n",
        "**Solution**\n",
        "\n",
        "Define a function that uses translate with a dictionary of punctuation characters:"
      ],
      "metadata": {
        "id": "tZs2-KR0WH56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "import sys\n",
        "import unicodedata"
      ],
      "metadata": {
        "id": "itBzmKs9VTg_"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create text\n",
        "text_data = [\n",
        "             'Hi!!!! I. Love. This. Song....',\n",
        "             '10000% Agree!!!! #LoveIT',\n",
        "             'Right?!?!']"
      ],
      "metadata": {
        "id": "rolQTP82XUOZ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_found = [re.findall(r'[a-zA-Z0-9]+', x) for x in text_data]\n",
        "word_found"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pl9fbFOMZmjY",
        "outputId": "eefb57a0-1871-4887-f182-7e99dbaa1dd9"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Hi', 'I', 'Love', 'This', 'Song'], ['10000', 'Agree', 'LoveIT'], ['Right']]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_data = [','.join(x) for x in word_found]"
      ],
      "metadata": {
        "id": "gf_-7jsSaLAq"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[string.replace(',', ' ') for string in new_data]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YmhxBG4c3sg",
        "outputId": "72953eab-f7e0-41ec-db56-b8126de060d7"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hi I Love This Song', '10000 Agree LoveIT', 'Right']"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.4 Tokenizing Text\n",
        "\n",
        "**Problem**\n",
        "\n",
        "You have text and want to break it up into individual words.\n",
        "\n",
        "**Solution**\n",
        "\n",
        "Natural Language Toolkit for Python (NLTK) has a powerful set of text manipulation\n",
        "operations, including word tokenizing:"
      ],
      "metadata": {
        "id": "mBAHrVXtd0HY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load library\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "vf5nZUysdqnm"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create text\n",
        "string = \"The science of today is the technology of tomorrow\""
      ],
      "metadata": {
        "id": "JZYCG6yfjiGg"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the library\n",
        "import nltk"
      ],
      "metadata": {
        "id": "Ie_awMSNkGuk"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dowload 'punkt\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2inFPkjkQXE",
        "outputId": "ec22ebe3-7f96-4a64-aa14-1df6e9247db8"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the string\n",
        "word_tokenize(string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c409pIqSkiVq",
        "outputId": "beb907ff-a99b-4ff7-975b-c96ee3d782fd"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'science', 'of', 'today', 'is', 'the', 'technology', 'of', 'tomorrow']"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(\"Bonjour tout le monde, je suis chez moi à la maison. Content de vous parler\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41QBZpp6lK3N",
        "outputId": "40992ff5-1892-40f6-b0a6-37f6b56c863d"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Bonjour',\n",
              " 'tout',\n",
              " 'le',\n",
              " 'monde',\n",
              " ',',\n",
              " 'je',\n",
              " 'suis',\n",
              " 'chez',\n",
              " 'moi',\n",
              " 'à',\n",
              " 'la',\n",
              " 'maison',\n",
              " '.',\n",
              " 'Content',\n",
              " 'de',\n",
              " 'vous',\n",
              " 'parler']"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also tokenize in sentence"
      ],
      "metadata": {
        "id": "Wcr6cPKnm2t8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "NI3IXtozmn0a"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize in the sentence\n",
        "sent_tokenize(string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5s0AzIKpm-Pi",
        "outputId": "ce3f5096-ce9f-4bd0-b7a8-c3baaa8e0683"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The science of today is the technology of tomorrow']"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.5 Removing Stop Word\n",
        "\n",
        "**Problem**\n",
        "\n",
        "Given tokenized text data, you want to remove extremely common words (e.g., a, is,\n",
        "of, on) that contain little informational value.\n",
        "\n",
        "After tokenize we can remove stop words\n",
        "**Solution**\n",
        "\n",
        "Use NLTK’s stopwords :"
      ],
      "metadata": {
        "id": "CZH5KBQqoC2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "xolpgx4znDbv"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NUHqThppffV",
        "outputId": "66315a0c-43dd-4911-ddbf-000e123f5ec6"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create word tokens\n",
        "tokenized_words = ['i',\n",
        "'am',\n",
        "'going',\n",
        "'to',\n",
        "'go',\n",
        "'to',\n",
        "'the',\n",
        "'store',\n",
        "'and',\n",
        "'park']"
      ],
      "metadata": {
        "id": "NcOohUzzqKac"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWxSI_b-qeNx",
        "outputId": "167d79b7-07b2-43d3-8322-6cf133a3744a"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'am', 'going', 'to', 'go', 'to', 'the', 'store', 'and', 'park']"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load stop words\n",
        "stop_words = stopwords.words('english')"
      ],
      "metadata": {
        "id": "Lh9a7QMQqf_x"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyxPTAkkqt3w",
        "outputId": "0ea8c8b9-d3f7-4925-c39a-a6e383095d11"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stop word\n",
        "[word for word in tokenized_words if word not in stop_words]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FP0Va5iq1gH",
        "outputId": "c04b018c-6024-43ca-d132-2a593b631f19"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['going', 'go', 'store', 'park']"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.6 Stemming Words(Mots radicaux)\n",
        "\n",
        "**Problème**\n",
        "\n",
        "Vous avez des mots symbolisés et souhaitez les convertir dans leurs formes racine.\n",
        "\n",
        "**Solution**\n",
        "\n",
        "Utiliser le PorterStemer de NLTK\n"
      ],
      "metadata": {
        "id": "qh5is1Pf93iy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oOZq4KH3rR63"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}